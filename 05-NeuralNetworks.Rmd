---
title: "Neural Networks"
author: "Albert Mata"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
subparagraph: true
output:
  pdf_document:
    latex_engine: xelatex
    includes:
      in_header: pdf-styles.sty
    toc: yes
    toc_depth: 3
    number_sections: false
  html_document:
    css: html-styles.css
    toc: yes
    toc_depth: 3    
    toc_float: yes
    number_sections: false
mainfont: "Times New Roman"
nocite: |
  @lantz2015machine
bibliography: bibliography.bib
---
\fontsize{12}{16} 
\selectfont

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("functions.R")
```

\newpage

# Black Box Methods - Neural Networks

We talk about __black box__ processes when the mechanism that transforms the input into the output is obfuscated by an imaginary box (mainly due to the complex mathematics allowing them to function). But it's dangerous to apply black box models blindly, so we'll peek inside the box and discover that neural networks mimic the structure of animal brains to model arbitrary functions.

## Understanding neural networks

An __Artificial Neural Network (ANN)__ models the relationship between a set of input signals and an output signal using a model derived from our understanding of how a biological brain responds to stimuli from sensory inputs. A brain uses a network of interconnected cells called __neurons__ to create a massive parallel processor. ANN uses a network of artificial neurons or __nodes__ to solve learning problems. ANNs are versatile learners currently used for:

* Speech and handwriting recognition programs.
* Automation of smart devices like self-driving cars and self-piloting drones.
* Scientific, social or economic phenomena such as sophisticated models of weather and climate patterns, tensile strength, fluid dynamics, etc.

ANNs are black box methods best applied to problems where the input data and output data are well-defined or fairly simple, yet the process that relates them is extremely complex.

### From biological to artificial neurons

Biological neurons:

1. Incoming signals are received by the cell's __dendrites__ and weighted according to relative importance or frequency.
2. As the __cell body__ accumulates these signals, at some point it transmits an output signal down the __axon__.
3. At the axon's terminals, the signal is passed to the neighboring neurons across a tiny gap known as a __synapse__.

Artificial neurons:

1. Input signals ($x_i$) are weighted ($w_i$) according to importance.
2. Input signals are summed by the cell body and the signal is passed on according to an __activation function__ ($f$) that generates the output signal ($y$).

$$output \ axon = y(x) = f \left( \sum_{i=1}^n w_i x_i\right)$$

ANNs use neurons as building blocks to construct complex models of data with these characteristics:

* An __activation function__ ($f$) transforms a neuron's combined input signals into a single output signal to be broadcasted further in the network.
* A __network topology__ (or architecture) describes the number of neurons and layers and how they are connected.
* A __training algorithm__ specifies how connection weights are set.

### Activation functions

A __threshold activation function__ is the mechanism by which the artifical neuron processes incoming information and passes it throughout the network only once a specified input threshold has been attained. For example, a neuron that fires only when the sum of input signals is at least zero (__unit step activation function__).

However, the activation functions used in ANN are chosen based on their ability to demonstrate desirable mathematical characteristics and accurately model relationships among data (i.e. the most commonly used S-shaped __sigmoid activation function__):

$$f(x) = \frac{1}{1 + e^{-x}}$$

There are different choices for activation functions: sigmoid, linear, saturated linear, hyperbolic tangent, gaussian... Each of them will result in a different neural network model that has strengths better suited for certain learning tasks.

To avoid __squashing problems__ (due to the fact that the range of input values that affect the output signal is relatively narrow) we standardize or normalize all neural network inputs so they fall within a small range around 0. This way we prevent large-valued features from dominating small-valued features. 

### Network topology

The ability of a neural network to learn more complex tasks is rooted in its topology, defined by:

* __The number of layers__. For example, in a __single-layer network__ some neurons called __input nodes__ receive unprocessed signals from the input data, process a single feature each and use their activation functions to send signals to the __output node__, which uses its own activation function to generate a final prediction. In this process there is only one set of connection weights ($w_1$ to $w_n$), that's why it's a single-layer network. But more sophisticated __multilayer networks__ are required for most learning tasks. In these multilayer networks, every node in one layer may or may not be connected to every node in the next layer.

* __Whether information is allowed to travel backward__. Networks in which the input signal is fed continously in one direction from connection to connection until it reaches the output layer are called __feedforward networks__. They offer a lot of flexibility (changing number of levels and nodes, modeling multiple outcomes, applying multiple hidden layers...). A neural network with multiple hidden layers is called a __Deep Neural Network (DNN)__ and its training is referred to as __deep learning__. Current de facto standard ANN topology is the __Multilayer Perceptron (MLP)__ (a multilayer feedforward network). *In contrast but still rarely used in practice, __recurrent networks__ (or __feedback networks__) allow signals to travel in both directions using loops and works for learning extremely complex patterns. They can add a short-term memory (__delay__) to immensely increase their power.* 

* __The number of nodes within each layer of the network__. The number of input nodes is predetermined by the number of features in the input data. The number of output nodes is predetermined by the number of outcomes to be modeled or the number of class levels in the outcome. But the number of hidden nodes is free and depends on many factors. In general, more complex topologies with a greater number of connections allow the learning of more complex problems. A greater number of nodes will result in a model that more closely mirrors the training data (at risk of overfitting and being too expensive and slow). In general, we'll want to use the fewest nodes that give adequate performance in a validation dataset.

### Training neural networks with backpropagation

As the neural network processes the input data, connections between the neurons are strengthened or weakened and their weights are adjusted to reflect the patterns observed over time. This is very computationally intensive, which is why we use the errors __backpropagation__ algorithm to train ANNs [@rumelhart1986learning].

So, multilayer feedforward networks that use the backpropagation algorithm are now common in the field of data mining.

__Strengths__:

* Can be adapted to classification or numeric prediction problems.
* Capable of modeling more complex patterns than nearly any algorithm.
* Makes few assumptions about the data's underlying relationships.

__Weaknesses__:

* Extremely computationally intensive and slow to train, particularly if the network topology is complex.
* Very prone to overfitting training data.
* Results in a complex black box model that is difficult, if not impossible, to interpret.

The backpropagation algorithm iterates through many cycles (or __epochs__) of two processes. As the network has no _a priori_ knowledge, starting weights are typically set at random. Then the algorithm iterates through these two phases util a stopping criterion is reached:

1. A __forward phase__ in which the neurons are activated in sequence from the input layer to the output layer, applying each neuron's weights and activation function along the way. Upon reaching the final layer, an output signal is produced.

2. A __backward phase__ in which the output is compared to the true target value in the training data. The difference between the two is the error that is propagated backwards in the network to modify the connection weights between neurons in order to reduce future errors. 

In order to know how much the weight for each neuron has to be changed, the algorithm uses a technique called __gradient descent__ (always look for the greatest downward slope). To do this, it uses the derivative of each neuron's activation function to identify the gradient in the direction of each of the incoming weights. The algorithm will attempt to change the weights that result in the greatest reduction in error by an amount known as the __learning rate__. The greater the learning rate, the faster the algorithm will attempt to descend down the gradients, which could reduce the training time at the risk of overshooting the valley.

## Example - modeling the strength of concrete with ANNs

### Step 1 - collecting data

We'll utilize data on the compressive strength of concrete [@yeh1998modeling].

This dataset includes 1,030 examples of concrete with some features describing the components used in the mixture (i.e. cement, slag, ash, water, superplasticizer, etc) and the aging time in days.

### Step 2 - exploring and preparing the data

```{r}
concrete <- read.csv("concrete.csv")
str(concrete)
```

We see values ranging anywhere from zero up to over a thousand, but we have already said that neural networks work best when the input data are scaled to a narrow range around zero. There exist two approaches to fix this:

* If the data follow a bell-shaped curve (a normal distribution) we can use R's built-in `scale()` function.
* If the data follow a uniform distribution or are severely nonnormal, normalization to a 0-1 range may be more appropriate.

We'll use the second approach here:

```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
```

We create a new variable (`concrete_norm`) because any transformation applied to the data prior to training the model will have to be applied in reverse later on in order to convert back to the original units of measurement. To facilitate this, it is a good idea to keep the original data (`concrete`).

Finally, we will partition the already randomly sorted data into a training set with 75 percent of the examples and a testing set with the remaining 25 percent:

```{r}
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
```

### Step 3 - training a model on the data

We will use a multilayer feedforward neural network from the `neuralnet` package to model the relationship between the ingredients used in concrete and the strength of the finished product. Other package with similar options are `nnet` and `RSNNS`.

Initially, we train the simplest multilayer feedforward network with only a single hidden node:

```{r fig.align="center", fig.height=4, fig.width=6}
library(neuralnet)
set.seed(12345)
concrete_model <- neuralnet(strength ~ cement + slag + ash + water + 
                              superplastic + coarseagg + fineagg + age, 
                            data = concrete_train)
plot(concrete_model, rep = "best")
```

We can see there is one input node for each of the eight features, followed by a single hidden node and a single output node that predicts the concrete strength. The graph also shows the weights for each of the connections and the __bias terms__ (in blue, much like the intercept in a linear equation, as a neural network with a single hidden node is somewhat similar to a linear regression model).

The error measure is the __Sum of Squared Errors (SSE)__ (the sum of the squared predicted minus actual values). The lower SSE, the better predictive performance. But this only talks about the model's performance on the training data.

### Step 4 - evaluating model performance

We use our model to generate predictions and then compare them to the true values:

```{r}
model_results <- compute(concrete_model, concrete_test[1:8])
```

The `compute()` function returns a list with two components: `$neurons`, which stores the neurons for each layer in the network, and `$net.result`, which stores the predicted values:

```{r}
predicted_strength <- model_results$net.result
```

We can't use a confusion matrix, as this is not a classification problem but a numeric prediction. What we do is to measure the correlation between our predicted values and the true ones:

\newpage

```{r}
cor(predicted_strength, concrete_test$strength)
```

Correlations close to 1 indicate strong linear relationships between two variables. A correlation here of about 0.806 indicates a fairly strong relationship. This implies our model is doing a fairly good job, even with only a single hidden node.

### Step 5 - improving model performance

But we can try to improve our results using a more complex topology:

```{r fig.align="center", fig.height=5.1, fig.width=6}
set.seed(12345)
concrete_model2 <- neuralnet(strength ~ cement + slag + ash + water + 
                               superplastic + coarseagg + fineagg + age, 
                             data = concrete_train, hidden = 5)
plot(concrete_model2, rep = "best")
```

The number of steps is much higher now (so computing this model is much more expensive). And we can also see the SSE is much lower now. But let's see how our model performs with unseen data:

```{r}
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
```

Correlation has gone from 0.806 to 0.924, which is a considerable improvement. We could try to improve even further trying different number of hidden nodes, applying different activation functions and so on.

# References
